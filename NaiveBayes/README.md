Naïve Bayes classifier is a probabilistic classifier which is based on Bayes’ theorem. It makes naive assumptions between the features. We train the data using given data and generate a probabilistic model using Bayes’ theorem and then apply that model to the new data we get. In the code above, I added an additional column to the train data, which tells us the class of the respective document. The word count matrix is built using the information available in the doc_ID column and the doc_class column. Then the probability of each word is found out. The probabilities of the classes are added to find the prediction matrix since we take log of the right hand side equation. The class with respect to which the maximum probability is achieved is probably the class of the document. Finally the code checks its results with the outputs provided. 
The trained Naïve Bayes classifier runs with a classification accuracy of 80.5730% for the given test data. A smoothing parameter of 0.1 is used in the code. The smoothing factor helps avoid zero probabilities. The code outputs the accuracy of the classifier along with the number of misclassifications that occur for each class. The run time of the code above is around 8 seconds on one of the university workstations. The run time is low because it is free of a couple of loops in finding the word count matrix that take way too much time to run. 
The third column of the misclass cell shown above is the number of misclassifications occurred in each class. Looking at the results, we can see that the misclassifications tend to happen more often for the miscellaneous documents such as ‘comp.os.ms-windows.misc’ – 150 misclassifications, ‘talk.politics.misc’ – 128 misclassifications, ‘talk.religion.misc’ – 109 misclassifications. Other topics such as ‘sci.electronics’ – 117 misclassifications, ‘comp.windows.x’ – 115 misclassifications, have misclassifications on the higher side as well because most of the vocabulary used in these topics is general. Topics related to specific sports, specific political topics, and a few science related topics such as medicine and space have particular words that only represent those topics. This makes it easier to classify such documents. 
The classification probabilities of the words make perfect sense when we look at them. The documents with higher probabilities of the words with lower word-IDs belong to lower classes and vice versa. This means that those words occur more in such topics than in others. 
